{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing RatingCounter.py\n"
     ]
    }
   ],
   "source": [
    "%%file RatingCounter.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRRatingCounter(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        (userID, movieId, rating, timestamp) = line.split('\\t')\n",
    "        yield rating, 1\n",
    "    def reducer(self, rating, occurences):\n",
    "        yield rating, sum(occurences)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRRatingCounter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/RatingCounter.prk.20160529.073518.579586\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/RatingCounter.prk.20160529.073518.579586/output...\n",
      "\"1\"\t6110\n",
      "\"2\"\t11370\n",
      "\"3\"\t27145\n",
      "\"4\"\t34174\n",
      "\"5\"\t21201\n",
      "Removing temp directory /tmp/RatingCounter.prk.20160529.073518.579586...\n"
     ]
    }
   ],
   "source": [
    "! python RatingCounter.py ml-100k/u.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FriendsByAge.py\n"
     ]
    }
   ],
   "source": [
    "%%file FriendsByAge.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRFriendsByAge(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        (ID, name, age, numFriends) = line.split(',')\n",
    "        yield age, float(numFriends)\n",
    "    def reducer(self, age, numFriends):\n",
    "        total = 0\n",
    "        numElements = 0\n",
    "        for x in numFriends:\n",
    "            total += x\n",
    "            numElements += 1\n",
    "        yield age, total/numElements\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRFriendsByAge.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/FriendsByAge.prk.20160529.090856.546668\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/FriendsByAge.prk.20160529.090856.546668/output...\n",
      "Removing temp directory /tmp/FriendsByAge.prk.20160529.090856.546668...\n"
     ]
    }
   ],
   "source": [
    "! python FriendsByAge.py fakefriends.csv > friendsbyage.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MinTemperatures.py\n"
     ]
    }
   ],
   "source": [
    "%%file MinTemperatures.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRMinTemperature(MRJob):\n",
    "    def MakeFahrenheit(self, tenthsOfCelsius):\n",
    "        celsius = float(tenthsOfCelsius)/10.0\n",
    "        fahrenheit = celsius * 1.8 + 32.0\n",
    "        return fahrenheit\n",
    "    def mapper(self, key, line):\n",
    "        (location, date, type, data, x, y, z, w) = line.split(',')\n",
    "        if(type=='TMIN'):\n",
    "            temperature = self.MakeFahrenheit(data)\n",
    "            yield location, temperature\n",
    "            \n",
    "    def reducer(self, location, temps):\n",
    "        yield location, min(temps)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRMinTemperature.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MinTemperatures.prk.20160529.091916.488311\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/MinTemperatures.prk.20160529.091916.488311/output...\n",
      "Removing temp directory /tmp/MinTemperatures.prk.20160529.091916.488311...\n"
     ]
    }
   ],
   "source": [
    "! python MinTemperatures.py 1800.csv > mintemps.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MaxTemperatures.py\n"
     ]
    }
   ],
   "source": [
    "%%file MaxTemperatures.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRMaxTemperature(MRJob):\n",
    "    def MakeFahrenheit(self, tenthsOfCelsius):\n",
    "        celsius = float(tenthsOfCelsius)/10.0\n",
    "        fahrenheit = celsius * 1.8 + 32.0\n",
    "        return fahrenheit\n",
    "    def mapper(self, key, line):\n",
    "        (location, date, type, data, x, y, z, w) = line.split(',')\n",
    "        if(type=='TMAX'):\n",
    "            temperature = self.MakeFahrenheit(data)\n",
    "            yield location, temperature\n",
    "            \n",
    "    def reducer(self, location, temps):\n",
    "        yield location, max(temps)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRMaxTemperature.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MaxTemperatures.prk.20160529.092328.866845\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/MaxTemperatures.prk.20160529.092328.866845/output...\n",
      "Removing temp directory /tmp/MaxTemperatures.prk.20160529.092328.866845...\n"
     ]
    }
   ],
   "source": [
    "! python MaxTemperatures.py 1800.csv > maxtemps.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordFrequency.py\n"
     ]
    }
   ],
   "source": [
    "%%file WordFrequency.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        words = line.split()#will split on any whitespaces (tab,space..)\n",
    "        for word in words:\n",
    "            word = unicode(word, \"utf-8\", errors=\"ignore\") #avoids issues in mrjob 5.0\n",
    "            yield word.lower(), 1\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/WordFrequency.prk.20160529.093521.721514\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/WordFrequency.prk.20160529.093521.721514/output...\n",
      "Removing temp directory /tmp/WordFrequency.prk.20160529.093521.721514...\n"
     ]
    }
   ],
   "source": [
    "!python WordFrequency.py book.txt > wordcount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordFrequencyBetter.py\n"
     ]
    }
   ],
   "source": [
    "%%file WordFrequencyBetter.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        words = WORD_REGEXP.findall(line)\n",
    "        for word in words:\n",
    "            word = unicode(word, \"utf-8\", errors=\"ignore\") #avoids issues in mrjob 5.0\n",
    "            yield word.lower(), 1\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/WordFrequencyBetter.prk.20160529.105145.601793\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/WordFrequencyBetter.prk.20160529.105145.601793/output...\n",
      "Removing temp directory /tmp/WordFrequencyBetter.prk.20160529.105145.601793...\n"
     ]
    }
   ],
   "source": [
    "!python WordFrequencyBetter.py book.txt > wordsbetter.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordFrequencySorted.py\n"
     ]
    }
   ],
   "source": [
    "%%file WordFrequencySorted.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "import re\n",
    "\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                  reducer=self.reducer_count_words),\n",
    "            MRStep(mapper=self.mapper_make_counts_key,\n",
    "                  reducer=self.reducer_output_words)\n",
    "        ]\n",
    "    def mapper_get_words(self, key, line):\n",
    "        words = WORD_REGEXP.findall(line)\n",
    "        for word in words:\n",
    "            word = unicode(word, \"utf-8\", errors=\"ignore\") #avoids issues in mrjob 5.0\n",
    "            yield word.lower(), 1\n",
    "    def reducer_count_words(self, key, values):\n",
    "        yield key, sum(values)\n",
    "    def mapper_make_counts_key(self,word,count):\n",
    "        yield '%04d'%int(count), word\n",
    "    def reducer_output_words(self, count, words):\n",
    "        for word in words:\n",
    "            yield count, word\n",
    "            \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/WordFrequencySorted.prk.20160529.120915.009967\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/WordFrequencySorted.prk.20160529.120915.009967/output...\n",
      "Removing temp directory /tmp/WordFrequencySorted.prk.20160529.120915.009967...\n"
     ]
    }
   ],
   "source": [
    "!python WordFrequencySorted.py book.txt > wordssorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MoneySpend.py\n"
     ]
    }
   ],
   "source": [
    "%%file MoneySpend.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRMoneyCounter(MRJob):\n",
    "    def mapper(self, key, line):\n",
    "        (userID, itemId, amount) = line.split(',')\n",
    "        yield userID, float(amount)\n",
    "    def reducer(self, userID, amount):\n",
    "        yield userID, sum(amount)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRMoneyCounter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MoneySpend.prk.20160529.141649.042485\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/MoneySpend.prk.20160529.141649.042485/output...\n",
      "Removing temp directory /tmp/MoneySpend.prk.20160529.141649.042485...\n"
     ]
    }
   ],
   "source": [
    "!python MoneySpend.py customer-orders.csv > moneyspend.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting AmountSpend.py\n"
     ]
    }
   ],
   "source": [
    "%%file AmountSpend.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.job import MRStep\n",
    "\n",
    "class MRMoneyCounter(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_amount,\n",
    "                  reducer=self.reducer_total_amount),\n",
    "            MRStep(mapper=self.mapper_make_amount_key,\n",
    "                  reducer=self.reducer_output_amount)\n",
    "        ]\n",
    "    def mapper_get_amount(self, key, line):\n",
    "        (userID, itemId, amount) = line.split(',')\n",
    "        yield userID, float(amount)\n",
    "    def reducer_total_amount(self, userID, amount):\n",
    "        yield userID, sum(amount)\n",
    "    def mapper_make_amount_key(self, userID, orderAmount):\n",
    "        yield '%04.02f'%float(orderAmount), userID\n",
    "    def reducer_output_amount(self, orderAmount, userIDs):\n",
    "        for userID in userIDs:\n",
    "            yield orderAmount, userID \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRMoneyCounter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/AmountSpend.prk.20160529.170900.398813\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/AmountSpend.prk.20160529.170900.398813/output...\n",
      "Removing temp directory /tmp/AmountSpend.prk.20160529.170900.398813...\n"
     ]
    }
   ],
   "source": [
    "!python AmountSpend.py customer-orders.csv > amountspend.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordFrequencyWithCombiner.py\n"
     ]
    }
   ],
   "source": [
    "%%file WordFrequencyWithCombiner.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "    def mapper(self,key,line):\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word = unicode(word, \"utf-8\", errors=\"ignore\")\n",
    "            yield word.lower(),1\n",
    "            \n",
    "    def combiner(self,key,values):#may not be called ..depends on hadoops decision\n",
    "        yield key,sum(values)\n",
    "        \n",
    "    def reducer(self,key,values):\n",
    "        yield key,sum(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/WordFrequencyWithCombiner.prk.20160529.172509.942843\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/WordFrequencyWithCombiner.prk.20160529.172509.942843/output...\n",
      "Removing temp directory /tmp/WordFrequencyWithCombiner.prk.20160529.172509.942843...\n"
     ]
    }
   ],
   "source": [
    "!python WordFrequencyWithCombiner.py book.txt > wordfrequencyCombiner.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostPopularMovie.py\n"
     ]
    }
   ],
   "source": [
    "%%file MostPopularMovie.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostPopularMovie(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_ratings,\n",
    "                  reducer=self.reducer_count_ratings),\n",
    "            MRStep(#mapper=self.mapper_passthrough,\n",
    "                   reducer=self.reducer_find_max)\n",
    "        ]\n",
    "    def mapper_get_ratings(self, key, line):\n",
    "        (userID, movieId, rating, timestamp) = line.split('\\t')\n",
    "        yield movieId, 1\n",
    "    def reducer_count_ratings(self, movieId, occurences):\n",
    "        yield None, (sum(occurences),movieId)\n",
    "    #def mapper_passthrough(self,key,values):\n",
    "    #    yield key,values\n",
    "    def reducer_find_max(self, key, values):\n",
    "        yield max(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MostPopularMovie.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MostPopularMovie.prk.20160529.173944.876325\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/MostPopularMovie.prk.20160529.173944.876325/output...\n",
      "583\t\"50\"\n",
      "Removing temp directory /tmp/MostPopularMovie.prk.20160529.173944.876325...\n"
     ]
    }
   ],
   "source": [
    "!python MostPopularMovie.py ml-100k/u.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostPopularMovieNice.py\n"
     ]
    }
   ],
   "source": [
    "%%file MostPopularMovieNice.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostPopularMovieNice(MRJob):\n",
    "    def configure_options(self):\n",
    "        super(MostPopularMovieNice, self).configure_options()\n",
    "        self.add_file_option('--items', help='Path to u.item')\n",
    "        #this file will be passed to every node of the job where it is needed\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_ratings,\n",
    "                   reducer_init=self.reducer_init,\n",
    "                   reducer=self.reducer_count_ratings),\n",
    "            MRStep(#mapper=self.mapper_passthrough,\n",
    "                   reducer=self.reducer_find_max)\n",
    "        ]\n",
    "    def mapper_get_ratings(self, key, line):\n",
    "        (userID, movieId, rating, timestamp) = line.split('\\t')\n",
    "        yield movieId, 1\n",
    "    def reducer_init(self):#run before our first reducer\n",
    "        self.movieNames = {}\n",
    "        with open(\"u.item\") as f:\n",
    "            for line in f:\n",
    "                fields = line.split('|')\n",
    "                self.movieNames[fields[0]] = fields[1].decode('utf-8','ignore')\n",
    "    def reducer_count_ratings(self, movieId, occurences):\n",
    "        yield None, (sum(occurences),self.movieNames[movieId])\n",
    "    #def mapper_passthrough(self,key,values):\n",
    "    #    yield key,values\n",
    "    def reducer_find_max(self, key, values):\n",
    "        yield max(values)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MostPopularMovieNice.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MostPopularMovieNice.prk.20160529.175358.910264\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/MostPopularMovieNice.prk.20160529.175358.910264/output...\n",
      "583\t\"Star Wars (1977)\"\n",
      "Removing temp directory /tmp/MostPopularMovieNice.prk.20160529.175358.910264...\n"
     ]
    }
   ],
   "source": [
    "!python MostPopularMovieNice.py --items=ml-100k/u.item ml-100k/u.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostPopularSuperHero.py\n"
     ]
    }
   ],
   "source": [
    "%%file MostPopularSuperHero.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MostPopularSuperHero(MRJob):\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MostPopularSuperHero, self).configure_options()\n",
    "        self.add_file_option('--names',help='Path to Marvel-names.txt')\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_friends_per_line,\n",
    "                  reducer=self.reducer_combine_friends),\n",
    "            MRStep(mapper=self.mapper_prep_for_sort,\n",
    "                  mapper_init=self.load_name_directory,\n",
    "                  reducer = self.reducer_find_max_friends)            \n",
    "        ]\n",
    "    \n",
    "    def mapper_count_friends_per_line(self,key,line):\n",
    "        fields = line.split()\n",
    "        heroID = fields[0]\n",
    "        numFriends = len(fields)-1\n",
    "        yield int(heroID),int(numFriends)\n",
    "        \n",
    "    def reducer_combine_friends(self,heroID,friendCounts):\n",
    "        yield heroID,sum(friendCounts)\n",
    "        \n",
    "    def mapper_prep_for_sort(self,heroID,friendCounts):\n",
    "        heroName = self.heroNames[heroID]\n",
    "        yield None, (friendCounts,heroName)\n",
    "        \n",
    "    def reducer_find_max_friends(self,key,value):\n",
    "        yield max(value)\n",
    "    \n",
    "    def load_name_directory(self):\n",
    "        self.heroNames = {}\n",
    "        \n",
    "        with open(\"Marvel-names.txt\") as f:\n",
    "            for line in f:\n",
    "                fields = line.split('\"')\n",
    "                heroID = int(fields[0])\n",
    "                self.heroNames[heroID] = fields[1].decode('utf-8','ignore')\n",
    "                \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostPopularSuperHero.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MostPopularSuperHero.prk.20160529.185831.785432\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/MostPopularSuperHero.prk.20160529.185831.785432/output...\n",
      "1933\t\"CAPTAIN AMERICA\"\n",
      "Removing temp directory /tmp/MostPopularSuperHero.prk.20160529.185831.785432...\n"
     ]
    }
   ],
   "source": [
    "!python MostPopularSuperHero.py --names=Marvel-names.txt Marvel-graph.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ProcessMarvel.py\n"
     ]
    }
   ],
   "source": [
    "%%file ProcessMarvel.py\n",
    "\n",
    "# Call this with one argument: the character ID you are starting from.\n",
    "# For example, Spider Man is 5306, The Hulk is 2548. Refer to Marvel-names.txt\n",
    "# for others.\n",
    "\n",
    "import sys\n",
    "\n",
    "print 'Creating BFS starting input for character ' + sys.argv[1]\n",
    "\n",
    "with open(\"BFS-iteration-0.txt\", 'w') as out:\n",
    "\n",
    "    with open(\"Marvel-graph.txt\") as f:\n",
    "\n",
    "        for line in f:\n",
    "            fields = line.split()\n",
    "            heroID = fields[0]\n",
    "            numConnections = len(fields) - 1\n",
    "            connections = fields[-numConnections:]\n",
    "\n",
    "            color = 'WHITE'\n",
    "            distance = 9999\n",
    "\n",
    "            if (heroID == sys.argv[1]) :\n",
    "                color = 'GRAY'\n",
    "                distance = 0\n",
    "\n",
    "            if (heroID != ''):\n",
    "                edges = ','.join(connections)\n",
    "                outStr = '|'.join((heroID, edges, str(distance), color))\n",
    "                out.write(outStr)\n",
    "                out.write(\"\\n\")\n",
    "\n",
    "\n",
    "    f.close()\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BFS starting input for character 2548\r\n"
     ]
    }
   ],
   "source": [
    "!python ProcessMarvel.py 2548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing BFSIteration.py\n"
     ]
    }
   ],
   "source": [
    "%%file BFSIteration.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.characterID = ''\n",
    "        self.connections = []\n",
    "        self.distance = 9999\n",
    "        self.color = 'WHITE'\n",
    "\n",
    "    #Format is ID|EDGES|DISTANCE|COLOR\n",
    "    def fromLine(self, line):\n",
    "        fields = line.split('|')\n",
    "        if (len(fields) == 4):\n",
    "            self.characterID = fields[0]\n",
    "            self.connections = fields[1].split(',')\n",
    "            self.distance = int(fields[2])\n",
    "            self.color = fields[3]\n",
    "\n",
    "    def getLine(self):\n",
    "        connections = ','.join(self.connections)\n",
    "        return '|'.join( (self.characterID, connections, str(self.distance), self.color) )\n",
    "\n",
    "class MRBFSIteration(MRJob):\n",
    "\n",
    "    #Normally when you run a MRJob the output is in JSON format\n",
    "    INPUT_PROTOCOL = RawValueProtocol\n",
    "    OUTPUT_PROTOCOL = RawValueProtocol\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MRBFSIteration, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--target', help=\"ID of character we are searching for\")\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        node = Node()\n",
    "        node.fromLine(line)\n",
    "        #If this node needs to be expanded...\n",
    "        if (node.color == 'GRAY'):\n",
    "            for connection in node.connections:\n",
    "                vnode = Node()\n",
    "                vnode.characterID = connection\n",
    "                vnode.distance = int(node.distance) + 1\n",
    "                vnode.color = 'GRAY'\n",
    "                if (self.options.target == connection):\n",
    "                    counterName = (\"Target ID \" + connection +\n",
    "                        \" was hit with distance \" + str(vnode.distance))\n",
    "                    self.increment_counter('Degrees of Separation',\n",
    "                        counterName, 1)\n",
    "                yield connection, vnode.getLine()\n",
    "\n",
    "            #We've processed this node, so color it black\n",
    "            node.color = 'BLACK'\n",
    "\n",
    "        #Emit the input node so we don't lose it.\n",
    "        yield node.characterID, node.getLine()\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        edges = []\n",
    "        distance = 9999\n",
    "        color = 'WHITE'\n",
    "\n",
    "        for value in values:\n",
    "            node = Node()\n",
    "            node.fromLine(value)\n",
    "\n",
    "            if (len(node.connections) > 0):\n",
    "                edges.extend(node.connections)\n",
    "\n",
    "            if (node.distance < distance):\n",
    "                distance = node.distance\n",
    "\n",
    "            if ( node.color == 'BLACK' ):\n",
    "                color = 'BLACK'\n",
    "\n",
    "            if ( node.color == 'GRAY' and color == 'WHITE' ):\n",
    "                color = 'GRAY'\n",
    "\n",
    "        node = Node()\n",
    "        node.characterID = key\n",
    "        node.distance = distance\n",
    "        node.color = color\n",
    "        node.connections = edges\n",
    "\n",
    "        yield key, node.getLine()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBFSIteration.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/BFSIteration.prk.20160530.075100.132042\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/BFSIteration.prk.20160530.075100.132042/output...\n",
      "Removing temp directory /tmp/BFSIteration.prk.20160530.075100.132042...\n"
     ]
    }
   ],
   "source": [
    "!python BFSIteration.py --target=100 BFS-iteration-0.txt > BFS-iteration-1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/BFSIteration.prk.20160530.075141.762876\n",
      "Running step 1 of 1...\n",
      "Counters: 1\n",
      "\tDegrees of Separation\n",
      "\t\tTarget ID 100 was hit with distance 2=2\n",
      "Counters: 1\n",
      "\tDegrees of Separation\n",
      "\t\tTarget ID 100 was hit with distance 2=2\n",
      "Streaming final output from /tmp/BFSIteration.prk.20160530.075141.762876/output...\n",
      "Removing temp directory /tmp/BFSIteration.prk.20160530.075141.762876...\n"
     ]
    }
   ],
   "source": [
    "!python BFSIteration.py --target=100 BFS-iteration-1.txt > BFS-iteration-2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MovieSimilarities.py\n"
     ]
    }
   ],
   "source": [
    "%%file MovieSimilarities.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from math import sqrt\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "class MovieSimilarities(MRJob):\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MovieSimilarities, self).configure_options()\n",
    "        self.add_file_option('--items', help='Path to u.item')\n",
    "\n",
    "    def load_movie_names(self):\n",
    "        # Load database of movie names.\n",
    "        self.movieNames = {}\n",
    "\n",
    "        with open(\"u.item\") as f:\n",
    "            for line in f:\n",
    "                fields = line.split('|')\n",
    "                self.movieNames[int(fields[0])] = fields[1].decode('utf-8', 'ignore')\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_parse_input,\n",
    "                    reducer=self.reducer_ratings_by_user),\n",
    "            MRStep(mapper=self.mapper_create_item_pairs,\n",
    "                    reducer=self.reducer_compute_similarity),\n",
    "            MRStep(mapper=self.mapper_sort_similarities,\n",
    "                    mapper_init=self.load_movie_names,\n",
    "                    reducer=self.reducer_output_similarities)]\n",
    "\n",
    "    def mapper_parse_input(self, key, line):\n",
    "        # Outputs userID => (movieID, rating)\n",
    "        (userID, movieID, rating, timestamp) = line.split('\\t')\n",
    "        yield  userID, (movieID, float(rating))\n",
    "\n",
    "    def reducer_ratings_by_user(self, user_id, itemRatings):\n",
    "        #Group (item, rating) pairs by userID\n",
    "\n",
    "        ratings = []\n",
    "        for movieID, rating in itemRatings:\n",
    "            ratings.append((movieID, rating))\n",
    "\n",
    "        yield user_id, ratings\n",
    "\n",
    "    def mapper_create_item_pairs(self, user_id, itemRatings):\n",
    "        # Find every pair of movies each user has seen, and emit\n",
    "        # each pair with its associated ratings\n",
    "\n",
    "        # \"combinations\" finds every possible pair from the list of movies\n",
    "        # this user viewed.\n",
    "        for itemRating1, itemRating2 in combinations(itemRatings, 2):\n",
    "            movieID1 = itemRating1[0]\n",
    "            rating1 = itemRating1[1]\n",
    "            movieID2 = itemRating2[0]\n",
    "            rating2 = itemRating2[1]\n",
    "\n",
    "            # Produce both orders so sims are bi-directional\n",
    "            yield (movieID1, movieID2), (rating1, rating2)\n",
    "            yield (movieID2, movieID1), (rating2, rating1)\n",
    "\n",
    "\n",
    "    def cosine_similarity(self, ratingPairs):\n",
    "        # Computes the cosine similarity metric between two\n",
    "        # rating vectors.\n",
    "        numPairs = 0\n",
    "        sum_xx = sum_yy = sum_xy = 0\n",
    "        for ratingX, ratingY in ratingPairs:\n",
    "            sum_xx += ratingX * ratingX\n",
    "            sum_yy += ratingY * ratingY\n",
    "            sum_xy += ratingX * ratingY\n",
    "            numPairs += 1\n",
    "\n",
    "        numerator = sum_xy\n",
    "        denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    "\n",
    "        score = 0\n",
    "        if (denominator):\n",
    "            score = (numerator / (float(denominator)))\n",
    "\n",
    "        return (score, numPairs)\n",
    "\n",
    "    def reducer_compute_similarity(self, moviePair, ratingPairs):\n",
    "        # Compute the similarity score between the ratings vectors\n",
    "        # for each movie pair viewed by multiple people\n",
    "\n",
    "        # Output movie pair => score, number of co-ratings\n",
    "\n",
    "        score, numPairs = self.cosine_similarity(ratingPairs)\n",
    "\n",
    "        # Enforce a minimum score and minimum number of co-ratings\n",
    "        # to ensure quality\n",
    "        if (numPairs > 10 and score > 0.95):\n",
    "            yield moviePair, (score, numPairs)\n",
    "\n",
    "    def mapper_sort_similarities(self, moviePair, scores):\n",
    "        # Shuffle things around so the key is (movie1, score)\n",
    "        # so we have meaningfully sorted results.\n",
    "        score, n = scores\n",
    "        movie1, movie2 = moviePair\n",
    "\n",
    "        yield (self.movieNames[int(movie1)], score), \\\n",
    "            (self.movieNames[int(movie2)], n)\n",
    "\n",
    "    def reducer_output_similarities(self, movieScore, similarN):\n",
    "        # Output the results.\n",
    "        # Movie => Similar Movie, score, number of co-ratings\n",
    "        movie1, score = movieScore\n",
    "        for movie2, n in similarN:\n",
    "            yield movie1, (movie2, score, n)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MovieSimilarities.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MovieSimilarities.prk.20160530.094002.129516\n",
      "Running step 1 of 3...\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "Streaming final output from /tmp/MovieSimilarities.prk.20160530.094002.129516/output...\n",
      "Removing temp directory /tmp/MovieSimilarities.prk.20160530.094002.129516...\n"
     ]
    }
   ],
   "source": [
    "!python MovieSimilarities.py --items=ml-100k/u.item ml-100k/u.data > sims.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
